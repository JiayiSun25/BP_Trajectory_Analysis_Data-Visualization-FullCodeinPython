{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65441c-1ce6-4fd8-afa9-3a269a3446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert StartDate format + create numeric SubjectKey_N while retaining original SubjectKey\n",
    "file_path = \"Desktop/extracted_data_0402.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"], errors='coerce')\n",
    "unique_subjects = df[\"SubjectKey\"].dropna().unique()\n",
    "subject_map = {old: str(i + 1) for i, old in enumerate(sorted(unique_subjects))}\n",
    "df[\"SubjectKey_N\"] = df[\"SubjectKey\"].map(subject_map)\n",
    "\n",
    "# Create patient_ID = SubjectKey_N + \"_\" + StartDate\n",
    "df[\"patient_ID\"] = df[\"SubjectKey_N\"] + \"_\" + df[\"StartDate\"].dt.strftime(\"%Y%m%d\")\n",
    "\n",
    "# Step 2: Filter BP values within 0–400\n",
    "df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors='coerce')\n",
    "df = df[(df[\"Value\"] >= 0) & (df[\"Value\"] <= 400)]\n",
    "\n",
    "# Step 3: Keep only patients aged 0–120\n",
    "df = df[(df[\"SubjectAgeAtEvent\"] >= 0) & (df[\"SubjectAgeAtEvent\"] <= 120)]\n",
    "\n",
    "# Step 4: Keep only systolic BP-related ItemOIDs\n",
    "systolic_itemoids = [\n",
    "    \"I_VISIT_PASD\", \"I_VISIT_PASN\",\n",
    "    \"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\",\n",
    "    \"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\",\n",
    "    \"I_VISIT_PAS_PIES_1\", \"I_VISIT_PAS_PIES_2\", \"I_VISIT_PAS_PIES_3\"\n",
    "]\n",
    "df = df[df[\"ItemOID\"].isin(systolic_itemoids)]\n",
    "\n",
    "# Step 5: Two-step priority filtering\n",
    "# Step 5.1: Keep PASD and PASN without conditions\n",
    "primary_df = df[df[\"ItemOID\"].isin([\"I_VISIT_PASD\", \"I_VISIT_PASN\"])]\n",
    "\n",
    "# Step 5.2: For the rest, keep based on priority by group\n",
    "def filter_secondary_items(group):\n",
    "    if group[\"ItemOID\"].isin([\"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\"]).any():\n",
    "        return group[group[\"ItemOID\"].isin([\"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\"])]\n",
    "    elif group[\"ItemOID\"].isin([\"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\"]).any():\n",
    "        return group[group[\"ItemOID\"].isin([\"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\"])]\n",
    "    else:\n",
    "        return group[group[\"ItemOID\"].isin([\"I_VISIT_PAS_PIES_1\", \"I_VISIT_PAS_PIES_2\", \"I_VISIT_PAS_PIES_3\"])]\n",
    "\n",
    "remaining_df = df[~df[\"ItemOID\"].isin([\"I_VISIT_PASD\", \"I_VISIT_PASN\"])]\n",
    "filtered_secondary_df = remaining_df.groupby(\"patient_ID\", group_keys=False).apply(filter_secondary_items)\n",
    "\n",
    "# Combine final filtered results\n",
    "df = pd.concat([primary_df, filtered_secondary_df], ignore_index=True)\n",
    "\n",
    "# Step 6: Sort visits by SubjectKey and StartDate\n",
    "df = df.sort_values(by=[\"SubjectKey\", \"StartDate\"]).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Remove patients with only one visit and print stats\n",
    "total_before_step7 = df.shape[0]\n",
    "visit_counts = df.groupby(\"SubjectKey\")[\"StartDate\"].nunique()\n",
    "valid_subjects = visit_counts[visit_counts > 1].index\n",
    "removed_subjects = visit_counts[visit_counts <= 1].shape[0]\n",
    "df = df[df[\"SubjectKey\"].isin(valid_subjects)]\n",
    "total_after_step7 = df.shape[0]\n",
    "print(f\"Step 7: Number of patients with only one visit removed: {removed_subjects}\")\n",
    "print(f\"Step 7: Corresponding number of rows removed: {total_before_step7 - total_after_step7}\")\n",
    "\n",
    "# Step 8: Remove rows under patient_IDs that do not contain complete item pairs/triplets\n",
    "def filter_complete_pairs(df, item_set):\n",
    "    \"\"\"\n",
    "    For each patient_ID, check if all required items in item_set exist.\n",
    "    If not all present, remove all such rows under that patient_ID.\n",
    "    \"\"\"\n",
    "    pid_group = df[df[\"ItemOID\"].isin(item_set)].groupby(\"patient_ID\")[\"ItemOID\"].apply(set)\n",
    "    valid_pids = pid_group[pid_group.apply(lambda x: set(item_set).issubset(x))].index\n",
    "    return df[~((df[\"ItemOID\"].isin(item_set)) & (~df[\"patient_ID\"].isin(valid_pids)))]\n",
    "\n",
    "# Apply to sitting, supine, and standing sets\n",
    "sitting_pair = [\"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\"]\n",
    "supine_pair = [\"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\"]\n",
    "standing_triplet = [\"I_VISIT_PAS_PIES_1\", \"I_VISIT_PAS_PIES_2\", \"I_VISIT_PAS_PIES_3\"]\n",
    "\n",
    "df = filter_complete_pairs(df, sitting_pair)\n",
    "df = filter_complete_pairs(df, supine_pair)\n",
    "df = filter_complete_pairs(df, standing_triplet)\n",
    "\n",
    "print(\"Step 8: Removed rows lacking complete systolic BP item sets\")\n",
    "\n",
    "# Step 9: Save to desktop\n",
    "output_path = \"Desktop/cleaned_data_0607.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b925c6e-a259-46af-bf1d-0a632138a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV1: extract rows with ItemOID = [\"I_VISIT_PASD\", \"I_VISIT_PASN\"]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned dataset\n",
    "file_path = \"Desktop/cleaned_data_0607.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step CSV1: filter rows where ItemOID is PASD or PASN\n",
    "csv1_df = df[df[\"ItemOID\"].isin([\"I_VISIT_PASD\", \"I_VISIT_PASN\"])]\n",
    "\n",
    "# Save the filtered result as CSV1\n",
    "csv1_path = \"Desktop/CSV1_PASD_or_PASN_only.csv\"\n",
    "csv1_df.to_csv(csv1_path, index=False)\n",
    "\n",
    "print(f\"CSV1 saved to: {csv1_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459dacb-acda-4e30-b847-5225fff70c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV2: extract all rows except those with ItemOID = [\"I_VISIT_PASD\", \"I_VISIT_PASN\"]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned dataset\n",
    "file_path = \"Desktop/cleaned_data_0607.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step CSV2: exclude rows where ItemOID is PASD or PASN, keep the rest\n",
    "csv2_df = df[~df[\"ItemOID\"].isin([\"I_VISIT_PASD\", \"I_VISIT_PASN\"])]\n",
    "\n",
    "# Save the result as CSV2\n",
    "csv2_path = \"Desktop/CSV2_exclude_PASD_PASN.csv\"\n",
    "csv2_df.to_csv(csv2_path, index=False)\n",
    "\n",
    "print(f\"CSV2 saved to: {csv2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecfce3-fe6a-4a30-81b8-49ca02e45f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV1: Keep only one row per patient_ID; prioritize I_VISIT_PASN; if not available, keep I_VISIT_PASD\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV1 file\n",
    "csv1_path = \"Desktop/CSV1_PASD_or_PASN_only.csv\"\n",
    "df = pd.read_csv(csv1_path)\n",
    "\n",
    "# Step 1: Prioritize I_VISIT_PASN\n",
    "pasn_df = df[df[\"ItemOID\"] == \"I_VISIT_PASN\"].drop_duplicates(subset=\"patient_ID\", keep=\"first\")\n",
    "\n",
    "# Step 2: For patient_IDs without PASN, keep PASD\n",
    "pasd_df = df[df[\"ItemOID\"] == \"I_VISIT_PASD\"]\n",
    "pasd_only_df = pasd_df[~pasd_df[\"patient_ID\"].isin(pasn_df[\"patient_ID\"])].drop_duplicates(subset=\"patient_ID\", keep=\"first\")\n",
    "\n",
    "# Step 3: Combine both sets\n",
    "final_df = pd.concat([pasn_df, pasd_only_df], ignore_index=True)\n",
    "\n",
    "# Save final CSV1\n",
    "output_path = \"Desktop/CSV1_final_one_row_per_patientID.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final CSV1 file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f04f74-34fc-4a6c-85eb-8455e617037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV1: Blood pressure categorization based on SBP, sex, and time of measurement (day vs. night)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the processed CSV1 file (one row per patient_ID, PASN prioritized)\n",
    "csv1_path = \"Desktop/CSV1_final_one_row_per_patientID.csv\"\n",
    "df = pd.read_csv(csv1_path)\n",
    "\n",
    "# Define classification function\n",
    "def classify_bp(row):\n",
    "    gender = row[\"Sex\"].strip().lower()\n",
    "    sbp = row[\"Value\"]\n",
    "    source = row[\"ItemOID\"]  # Identify PASN (night) or PASD (day)\n",
    "\n",
    "    if pd.isna(sbp):\n",
    "        return \"Unclassified\"\n",
    "\n",
    "    # Night SBP rules (PASN)\n",
    "    if source == \"I_VISIT_PASN\":\n",
    "        if gender == \"m\":\n",
    "            if sbp >= 120:\n",
    "                return \"High BP\"\n",
    "            elif sbp < 97:\n",
    "                return \"Low BP\"\n",
    "            else:\n",
    "                return \"Normal BP\"\n",
    "        elif gender == \"f\":\n",
    "            if sbp >= 120:\n",
    "                return \"High BP\"\n",
    "            elif sbp < 92:\n",
    "                return \"Low BP\"\n",
    "            else:\n",
    "                return \"Normal BP\"\n",
    "    \n",
    "    # Day SBP rules (PASD)\n",
    "    elif source == \"I_VISIT_PASD\":\n",
    "        if gender == \"m\":\n",
    "            if sbp >= 135:\n",
    "                return \"High BP\"\n",
    "            elif sbp < 115:\n",
    "                return \"Low BP\"\n",
    "            else:\n",
    "                return \"Normal BP\"\n",
    "        elif gender == \"f\":\n",
    "            if sbp >= 135:\n",
    "                return \"High BP\"\n",
    "            elif sbp < 105:\n",
    "                return \"Low BP\"\n",
    "            else:\n",
    "                return \"Normal BP\"\n",
    "    \n",
    "    return \"Unclassified\"\n",
    "\n",
    "# Apply classification\n",
    "df[\"BP_Category\"] = df.apply(classify_bp, axis=1)\n",
    "\n",
    "# Save output\n",
    "output_path = \"Desktop/CSV1_with_BP_Category_Final.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Blood pressure classification completed and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f3efa-05d1-4357-8463-80019e56998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV2: Compute average SBP from valid composite measurements (sitting/supine/standing)\n",
    "# One average row is added per patient_ID based on available full sets.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV2 file\n",
    "csv2_path = \"Desktop/CSV2_exclude_PASD_PASN.csv\"\n",
    "df = pd.read_csv(csv2_path)\n",
    "\n",
    "# Define three composite groups\n",
    "groups = {\n",
    "    \"sitting_average\": [\"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\"],\n",
    "    \"supine_average\": [\"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\"],\n",
    "    \"standing_average\": [\"I_VISIT_PAS_PIES_1\", \"I_VISIT_PAS_PIES_2\", \"I_VISIT_PAS_PIES_3\"]\n",
    "}\n",
    "\n",
    "# Collect new average rows\n",
    "new_rows = []\n",
    "\n",
    "# Group by patient_ID\n",
    "for pid, group in df.groupby(\"patient_ID\"):\n",
    "    for new_itemoid, itemoid_list in groups.items():\n",
    "        subset = group[group[\"ItemOID\"].isin(itemoid_list)]\n",
    "        if set(itemoid_list).issubset(set(subset[\"ItemOID\"])):  # Check if all required items are present\n",
    "            avg_value = subset[\"Value\"].mean()\n",
    "            ref_row = subset.iloc[0].copy()\n",
    "            ref_row[\"ItemOID\"] = new_itemoid\n",
    "            ref_row[\"Value\"] = avg_value\n",
    "            new_rows.append(ref_row)\n",
    "            break  # Only keep one average group (in defined priority order)\n",
    "\n",
    "# Combine original data with new average rows\n",
    "if new_rows:\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    final_df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = df.copy()\n",
    "\n",
    "# Save final result\n",
    "output_path = \"Desktop/CSV2_with_averages.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Average computation completed. Final file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3f098-a3b6-4bbd-80be-617d299ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sitting group\n",
    "sitting_items = [\"I_VISIT_SBP_SITTING\", \"I_VISIT_SBP_SITTING2\"]\n",
    "\n",
    "# Count number of records for each patient_ID in the sitting group\n",
    "sitting_counts = df[df[\"ItemOID\"].isin(sitting_items)].groupby(\"patient_ID\").size()\n",
    "\n",
    "# Identify patient_IDs with more than 2 records (indicating duplicates)\n",
    "sitting_duplicates = sitting_counts[sitting_counts > 2]\n",
    "\n",
    "# Print count and sample\n",
    "print(f\"There are {len(sitting_duplicates)} patient_IDs with duplicate records in the sitting group.\")\n",
    "print(sitting_duplicates.head())\n",
    "\n",
    "# Check duplicates considering both ItemOID and patient_ID\n",
    "repeated_items = df[df[\"ItemOID\"].isin(sitting_items)]\n",
    "repeated_detail = repeated_items.groupby([\"patient_ID\", \"ItemOID\"]).size().reset_index(name=\"count\")\n",
    "repeated_detail = repeated_detail[repeated_detail[\"count\"] > 1]\n",
    "\n",
    "# Print duplicate details\n",
    "print(\"Duplicate record details:\")\n",
    "print(repeated_detail.head())\n",
    "\n",
    "# Define supine group\n",
    "supine_items = [\"I_VISIT_PAS_SUPINO\", \"I_VISIT_PAS_SUP_2\"]\n",
    "\n",
    "# Count supine records per patient_ID\n",
    "supine_counts = df[df[\"ItemOID\"].isin(supine_items)].groupby(\"patient_ID\").size()\n",
    "supine_duplicates = supine_counts[supine_counts > 2]\n",
    "\n",
    "# Print results\n",
    "print(f\"There are {len(supine_duplicates)} patient_IDs with duplicate records in the supine group.\")\n",
    "print(supine_duplicates.head())\n",
    "\n",
    "# Define standing group\n",
    "standing_items = [\"I_VISIT_PAS_PIES_1\", \"I_VISIT_PAS_PIES_2\", \"I_VISIT_PAS_PIES_3\"]\n",
    "\n",
    "# Count standing records per patient_ID\n",
    "standing_counts = df[df[\"ItemOID\"].isin(standing_items)].groupby(\"patient_ID\").size()\n",
    "standing_duplicates = standing_counts[standing_counts > 3]\n",
    "\n",
    "# Print results\n",
    "print(f\"There are {len(standing_duplicates)} patient_IDs with duplicate records in the standing group.\")\n",
    "print(standing_duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68145637-81d6-4281-8a3d-a88ae2ecc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV2: Label SBP categories for composite average rows only (sitting/supine/standing)\n",
    "# Classification is based on predefined SBP thresholds.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file that contains average rows\n",
    "csv_path = \"Desktop/CSV2_with_averages.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 1: Keep only the three types of average rows\n",
    "average_df = df[df[\"ItemOID\"].isin([\"sitting_average\", \"supine_average\", \"standing_average\"])].copy()\n",
    "\n",
    "# Step 2: Add BP_Category column based on average value\n",
    "def classify_secondary_bp(value):\n",
    "    if pd.isna(value):\n",
    "        return \"Unclassified\"\n",
    "    elif value >= 140:\n",
    "        return \"High BP\"\n",
    "    elif value < 90:\n",
    "        return \"Low BP\"\n",
    "    else:\n",
    "        return \"Normal BP\"\n",
    "\n",
    "average_df[\"BP_Category\"] = average_df[\"Value\"].apply(classify_secondary_bp)\n",
    "\n",
    "# Step 3: Save the new file\n",
    "output_path = \"Desktop/CSV2_average_with_BP_Category.csv\"\n",
    "average_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Secondary BP labeling completed. File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912ea9b-b7bc-46e7-aeb6-64a0c769c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CSV1 and CSV2 to create final CSV3 with complete BP classification per patient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read cleaned CSV1 and CSV2 files\n",
    "csv1 = pd.read_csv(\"Desktop/CSV1_with_BP_Category_Final.csv\")\n",
    "csv2 = pd.read_csv(\"Desktop/CSV2_average_with_BP_Category.csv\")\n",
    "\n",
    "# Drop unused columns if they exist\n",
    "csv1 = csv1.drop(columns=[\"StudyEventRepeatKey\"], errors=\"ignore\")\n",
    "csv2 = csv2.drop(columns=[\"StudyEventRepeatKey\"], errors=\"ignore\")\n",
    "\n",
    "# Rename columns to prevent conflicts after merge\n",
    "csv1 = csv1.rename(columns={\n",
    "    \"ItemOID\": \"ItemOID_CSV1\",\n",
    "    \"Value\": \"Value_CSV1\",\n",
    "    \"BP_Category\": \"BP_Category_CSV1\",\n",
    "    \"SubjectKey\": \"SubjectKey_CSV1\",\n",
    "    \"Sex\": \"Sex_CSV1\",\n",
    "    \"SubjectAgeAtEvent\": \"SubjectAgeAtEvent_CSV1\",\n",
    "    \"StartDate\": \"StartDate_CSV1\",\n",
    "    \"SubjectKey_N\": \"SubjectKey_N_CSV1\"\n",
    "})\n",
    "\n",
    "csv2 = csv2.rename(columns={\n",
    "    \"ItemOID\": \"ItemOID_CSV2\",\n",
    "    \"Value\": \"Value_CSV2\",\n",
    "    \"BP_Category\": \"BP_Category_CSV2\",\n",
    "    \"SubjectKey\": \"SubjectKey_CSV2\",\n",
    "    \"Sex\": \"Sex_CSV2\",\n",
    "    \"SubjectAgeAtEvent\": \"SubjectAgeAtEvent_CSV2\",\n",
    "    \"StartDate\": \"StartDate_CSV2\",\n",
    "    \"SubjectKey_N\": \"SubjectKey_N_CSV2\"\n",
    "})\n",
    "\n",
    "# Select relevant columns from both datasets\n",
    "csv1_base = csv1[[\"patient_ID\", \"SubjectKey_CSV1\", \"Sex_CSV1\", \"SubjectAgeAtEvent_CSV1\", \"StartDate_CSV1\", \"SubjectKey_N_CSV1\",\n",
    "                  \"ItemOID_CSV1\", \"Value_CSV1\", \"BP_Category_CSV1\"]]\n",
    "csv2_base = csv2[[\"patient_ID\", \"SubjectKey_CSV2\", \"Sex_CSV2\", \"SubjectAgeAtEvent_CSV2\", \"StartDate_CSV2\", \"SubjectKey_N_CSV2\",\n",
    "                  \"ItemOID_CSV2\", \"Value_CSV2\", \"BP_Category_CSV2\"]]\n",
    "\n",
    "# Merge by patient_ID\n",
    "merged = pd.merge(csv1_base, csv2_base, on=\"patient_ID\", how=\"outer\")\n",
    "\n",
    "# Fill demographic fields using CSV1 first, fallback to CSV2\n",
    "merged[\"SubjectKey\"] = merged[\"SubjectKey_CSV1\"].combine_first(merged[\"SubjectKey_CSV2\"])\n",
    "merged[\"Sex\"] = merged[\"Sex_CSV1\"].combine_first(merged[\"Sex_CSV2\"])\n",
    "merged[\"SubjectAgeAtEvent\"] = merged[\"SubjectAgeAtEvent_CSV1\"].combine_first(merged[\"SubjectAgeAtEvent_CSV2\"])\n",
    "merged[\"StartDate\"] = merged[\"StartDate_CSV1\"].combine_first(merged[\"StartDate_CSV2\"])\n",
    "merged[\"SubjectKey_N\"] = merged[\"SubjectKey_N_CSV1\"].combine_first(merged[\"SubjectKey_N_CSV2\"])\n",
    "\n",
    "# Keep final cleaned columns\n",
    "final_df = merged[[\n",
    "    \"patient_ID\", \"SubjectKey\", \"Sex\", \"SubjectAgeAtEvent\", \"StartDate\", \"SubjectKey_N\",\n",
    "    \"ItemOID_CSV1\", \"Value_CSV1\", \"BP_Category_CSV1\",\n",
    "    \"ItemOID_CSV2\", \"Value_CSV2\", \"BP_Category_CSV2\"\n",
    "]]\n",
    "\n",
    "# Save output\n",
    "output_path = \"Desktop/CSV3_merged_all_patients_filled.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV3 merge and repair completed. File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9abd0-1f79-4658-adf1-e4f1099bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add BP_Classification based on BP_Category_CSV1 and BP_Category_CSV2 using a given Classification map\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV3 file\n",
    "csv3_path = \"Desktop/CSV3_merged_all_patients_filled.csv\"\n",
    "df = pd.read_csv(csv3_path)\n",
    "\n",
    "# Define classification mapping based on CSV1 and CSV2 categories\n",
    "classification_map = {\n",
    "    (\"High BP\", \"High BP\"): \"High BP\",\n",
    "    (\"High BP\", \"Normal BP\"): \"Masked Hypertension\",\n",
    "    (\"High BP\", \"Low BP\"): \"Masked Hypertension\",\n",
    "    (\"Normal BP\", \"High BP\"): \"High BP White coat\",\n",
    "    (\"Normal BP\", \"Normal BP\"): \"Normal BP\",\n",
    "    (\"Normal BP\", \"Low BP\"): \"Whitecoat Low blood pressure\",\n",
    "    (\"Low BP\", \"High BP\"): \"High BP White coat\",\n",
    "    (\"Low BP\", \"Normal BP\"): \"Masked Low blood pressure\",\n",
    "    (\"Low BP\", \"Low BP\"): \"Low BP\"\n",
    "}\n",
    "\n",
    "# Classification function based on availability of both categories\n",
    "def classify(row):\n",
    "    cat1 = row[\"BP_Category_CSV1\"]\n",
    "    cat2 = row[\"BP_Category_CSV2\"]\n",
    "\n",
    "    if pd.notna(cat1) and pd.isna(cat2):\n",
    "        return cat1\n",
    "    elif pd.notna(cat1) and pd.notna(cat2):\n",
    "        return classification_map.get((cat1, cat2), \"Unclassified\")\n",
    "    else:\n",
    "        return \"Unclassified\"\n",
    "\n",
    "# Apply the classification\n",
    "df[\"BP_Classification\"] = df.apply(classify, axis=1)\n",
    "\n",
    "# Save final result\n",
    "output_path = \"Desktop/CSV3_with_BP_Classification_final.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final classification saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62646ac5-e9e4-47f2-ba8d-379dd11ef48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV3: Generate trajectory and calculate transition age and time gap based on BP_Classification\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "file_path = \"CSV3_BP_Matching_Final_With_Classification.csv\"\n",
    "# This file is generated later from classification code after data matching and cleaning\n",
    "# See: 'Label BP_Classification based on BP_Category_CSV1 and CSV2 (Final Classification)'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure StartDate is in datetime format\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\n",
    "\n",
    "# Sort by SubjectKey and StartDate\n",
    "df_sorted = df.sort_values(by=[\"SubjectKey\", \"StartDate\"]).reset_index(drop=True)\n",
    "\n",
    "# Initialize new columns\n",
    "df_sorted[\"trajectory\"] = \"\"\n",
    "df_sorted[\"transition_age\"] = None\n",
    "df_sorted[\"time_dif_visit_Days\"] = None\n",
    "\n",
    "# Process each SubjectKey group\n",
    "for subject, group in df_sorted.groupby(\"SubjectKey\"):\n",
    "    group = group.sort_values(by=\"StartDate\").reset_index()\n",
    "    for i in range(1, len(group)):\n",
    "        idx = group.loc[i, \"index\"]\n",
    "        current_bp = group.loc[i, \"BP_Classification\"]\n",
    "        previous_bp = group.loc[i - 1, \"BP_Classification\"]\n",
    "        date_1 = group.loc[i, \"StartDate\"]\n",
    "        date_2 = group.loc[i - 1, \"StartDate\"]\n",
    "        age_1 = group.loc[i, \"SubjectAgeAtEvent\"]\n",
    "\n",
    "        df_sorted.at[idx, \"trajectory\"] = f\"{previous_bp} - {current_bp}\"\n",
    "        df_sorted.at[idx, \"transition_age\"] = age_1\n",
    "        df_sorted.at[idx, \"time_dif_visit_Days\"] = (date_1 - date_2).days\n",
    "\n",
    "# Save result\n",
    "output_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"transition_age uses the second visit's age. File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929a517-b7b0-4950-97e7-3ec0fa399b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot age distribution for four specific BP classifications using boxplot and violin plot\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter for four target BP classifications\n",
    "target_classes = [\n",
    "    \"Masked Hypertension\",\n",
    "    \"High BP White coat\",\n",
    "    \"Whitecoat Low BP\",\n",
    "    \"Masked Low BP\"\n",
    "]\n",
    "df_filtered = df[df[\"BP_Classification\"].isin(target_classes)]\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot of transition_age by BP_Classification\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"BP_Classification\", y=\"SubjectAgeAtEvent\", data=df_filtered, palette=\"Set2\")\n",
    "plt.title(\"Boxplot of Age by BP Classification\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Desktop/boxplot_BP_Classification_age.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create violin plot of transition_age by BP_Classification\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x=\"BP_Classification\", y=\"SubjectAgeAtEvent\", data=df_filtered, inner=\"box\", palette=\"Set2\")\n",
    "plt.title(\"Violin Plot of Age by BP Classification\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Desktop/violinplot_BP_Classification_age.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37af6a8-bb9c-4ff6-be9b-12367e703545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plots of age vs. time_dif_visit_Days for each trajectory group\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure time_dif_visit_Days is numeric\n",
    "df[\"time_dif_visit_Days\"] = pd.to_numeric(df[\"time_dif_visit_Days\"], errors=\"coerce\")\n",
    "\n",
    "# Define trajectory types and associated colors\n",
    "trajectory_list = [\n",
    "    (\"High BP - Normal BP\", \"blue\"),\n",
    "    (\"High BP - Low BP\", \"orange\"),\n",
    "    (\"High BP - High BP\", \"gold\"),\n",
    "    (\"Normal BP - High BP\", \"purple\"),\n",
    "    (\"Normal BP - Low BP\", \"green\"),\n",
    "    (\"Normal BP - Normal BP\", \"gray\"),\n",
    "    (\"Low BP - High BP\", \"brown\"),\n",
    "    (\"Low BP - Normal BP\", \"red\"),\n",
    "    (\"Low BP - Low BP\", \"teal\")\n",
    "]\n",
    "\n",
    "# Generate scatter plots for each trajectory type\n",
    "for trajectory_type, color in trajectory_list:\n",
    "    filtered_df = df[df[\"trajectory\"] == trajectory_type].dropna(subset=[\"time_dif_visit_Days\", \"age\"])\n",
    "    n = len(filtered_df)\n",
    "\n",
    "    if n == 0:\n",
    "        print(f\"No data for trajectory: {trajectory_type}\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(filtered_df[\"age\"], filtered_df[\"time_dif_visit_Days\"], color=color, alpha=0.6)\n",
    "\n",
    "    max_y = filtered_df[\"time_dif_visit_Days\"].max()\n",
    "    yticks = np.arange(0, max_y + 500, 500)\n",
    "    plt.yticks(yticks)\n",
    "\n",
    "    plt.title(f\"{trajectory_type} (n = {n})\\nage vs. time_dif_visit_Days\")\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Time Difference Between Visits (Days)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75476cfa-e6bf-4cd3-9a8d-a453933e5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot showing transition_age by 9 BP trajectory types using Seaborn Set2 palette\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Desktop/CSV3_BP_Matching_With_Trajectory.csv\")\n",
    "df = df.dropna(subset=[\"trajectory\", \"transition_age\"])\n",
    "\n",
    "# Keep only the 9 defined BP trajectory types\n",
    "valid_trajectories = [\n",
    "    \"High BP - Normal BP\", \"High BP - Low BP\", \"High BP - High BP\",\n",
    "    \"Normal BP - High BP\", \"Normal BP - Low BP\", \"Normal BP - Normal BP\",\n",
    "    \"Low BP - High BP\", \"Low BP - Normal BP\", \"Low BP - Low BP\"\n",
    "]\n",
    "df = df[df[\"trajectory\"].isin(valid_trajectories)]\n",
    "\n",
    "# Count samples for each trajectory type\n",
    "trajectory_counts = df[\"trajectory\"].value_counts().to_dict()\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.boxplot(x=\"trajectory\", y=\"transition_age\", data=df, palette=\"Set2\")\n",
    "\n",
    "# Add sample size (n=) labels\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    traj = label.get_text()\n",
    "    count = trajectory_counts.get(traj, 0)\n",
    "    ax.text(i, ax.get_ylim()[0] - 1, f\"n={count}\", ha='center', va='top', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Transition Age by BP Trajectory Type\")\n",
    "plt.xlabel(\"BP Trajectory\")\n",
    "plt.ylabel(\"Transition Age\")\n",
    "if ax.get_legend():\n",
    "    ax.legend_.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748976a6-9808-48b4-9edc-9b67d5140670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin + strip plots for transition_age across 9 BP trajectory types using Set2 palette\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Desktop/CSV3_BP_Matching_With_Trajectory.csv\")\n",
    "df = df.dropna(subset=[\"trajectory\", \"transition_age\"])\n",
    "\n",
    "# Keep only 9 specific trajectory types\n",
    "valid_trajectories = [\n",
    "    \"High BP - Normal BP\", \"High BP - Low BP\", \"High BP - High BP\",\n",
    "    \"Normal BP - High BP\", \"Normal BP - Low BP\", \"Normal BP - Normal BP\",\n",
    "    \"Low BP - High BP\", \"Low BP - Normal BP\", \"Low BP - Low BP\"\n",
    "]\n",
    "df = df[df[\"trajectory\"].isin(valid_trajectories)]\n",
    "\n",
    "# Count samples per trajectory\n",
    "trajectory_counts = df[\"trajectory\"].value_counts().to_dict()\n",
    "\n",
    "# Create violin and strip plots\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.violinplot(x=\"trajectory\", y=\"transition_age\", data=df, palette=\"Set2\", inner=None, dodge=False)\n",
    "sns.stripplot(x=\"trajectory\", y=\"transition_age\", data=df, color=\"black\", alpha=0.3, size=2, jitter=True)\n",
    "\n",
    "# Add sample size (n=) labels\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    traj = label.get_text()\n",
    "    count = trajectory_counts.get(traj, 0)\n",
    "    ax.text(i, ax.get_ylim()[0] - 1, f\"n={count}\", ha='center', va='top', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Transition Age by BP Trajectory Type (Violin + Strip, Set2 Palette)\")\n",
    "plt.xlabel(\"BP Trajectory\")\n",
    "plt.ylabel(\"Transition Age\")\n",
    "if ax.get_legend():\n",
    "    ax.legend_.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fbf7d-c958-40b9-bf14-a656efd80c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 9 BP trajectory groups as individual scatter plots in a 3x3 grid layout\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Load data\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure numeric format\n",
    "df[\"time_dif_visit_Days\"] = pd.to_numeric(df[\"time_dif_visit_Days\"], errors=\"coerce\")\n",
    "\n",
    "# Define trajectory types and colors\n",
    "trajectory_list = [\n",
    "    (\"High BP - Normal BP\", \"blue\"),\n",
    "    (\"High BP - Low BP\", \"orange\"),\n",
    "    (\"High BP - High BP\", \"gold\"),\n",
    "    (\"Normal BP - High BP\", \"purple\"),\n",
    "    (\"Normal BP - Low BP\", \"green\"),\n",
    "    (\"Normal BP - Normal BP\", \"gray\"),\n",
    "    (\"Low BP - High BP\", \"brown\"),\n",
    "    (\"Low BP - Normal BP\", \"red\"),\n",
    "    (\"Low BP - Low BP\", \"teal\")\n",
    "]\n",
    "\n",
    "# Determine global axis limits\n",
    "all_transition_age = df[\"transition_age\"].dropna()\n",
    "all_time_diff = df[\"time_dif_visit_Days\"].dropna()\n",
    "x_min = all_transition_age.min() - 2\n",
    "x_max = all_transition_age.max() + 2\n",
    "y_min = 0\n",
    "y_max = all_time_diff.max() + 500\n",
    "\n",
    "# Create 3x3 subplot grid\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "gs = gridspec.GridSpec(3, 3, figure=fig)\n",
    "\n",
    "for i, (trajectory_type, color) in enumerate(trajectory_list):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    \n",
    "    filtered_df = df[df[\"trajectory\"] == trajectory_type].dropna(subset=[\"time_dif_visit_Days\", \"transition_age\"])\n",
    "    n = len(filtered_df)\n",
    "    \n",
    "    if n > 0:\n",
    "        ax.scatter(filtered_df[\"transition_age\"], filtered_df[\"time_dif_visit_Days\"],\n",
    "                   color=color, alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f\"{trajectory_type} (n={n})\", fontsize=12)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(\"Transition Age\")\n",
    "    ax.set_ylabel(\"Time Difference (Days)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle(\"Blood Pressure Trajectory Patterns (Original Classification: All 9 Groups)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3925e-7b48-4e3c-8a65-df4c076b5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pie charts of time difference distribution for each BP trajectory group\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure numeric\n",
    "df[\"time_dif_visit_Days\"] = pd.to_numeric(df[\"time_dif_visit_Days\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"trajectory\", \"time_dif_visit_Days\"])\n",
    "\n",
    "# Time difference bucket function\n",
    "def categorize_gap(days):\n",
    "    if days <= 30:\n",
    "        return \"≤30 days\"\n",
    "    elif days <= 90:\n",
    "        return \"31–90 days\"\n",
    "    elif days <= 180:\n",
    "        return \"91–180 days\"\n",
    "    elif days <= 365:\n",
    "        return \"181–365 days\"\n",
    "    elif days <= 730:\n",
    "        return \"1–2 years\"\n",
    "    elif days <= 1095:\n",
    "        return \"2–3 years\"\n",
    "    else:\n",
    "        return \"≥3 years\"\n",
    "\n",
    "# Define trajectory groups\n",
    "trajectory_list = [\n",
    "    \"High BP - Normal BP\",\n",
    "    \"High BP - Low BP\",\n",
    "    \"High BP - High BP\",\n",
    "    \"Normal BP - High BP\",\n",
    "    \"Normal BP - Low BP\",\n",
    "    \"Normal BP - Normal BP\",\n",
    "    \"Low BP - High BP\",\n",
    "    \"Low BP - Normal BP\",\n",
    "    \"Low BP - Low BP\"\n",
    "]\n",
    "\n",
    "# Generate pie chart for each trajectory\n",
    "for trajectory in trajectory_list:\n",
    "    subset = df[df[\"trajectory\"] == trajectory]\n",
    "    n = len(subset)\n",
    "    if n == 0:\n",
    "        continue\n",
    "\n",
    "    # Bucket time difference\n",
    "    subset[\"gap_category\"] = subset[\"time_dif_visit_Days\"].apply(categorize_gap)\n",
    "    counts = subset[\"gap_category\"].value_counts().sort_index()\n",
    "    \n",
    "    # Labels with count and percent\n",
    "    labels = [f\"{label} (n={count}, {count/n*100:.1f}%)\" for label, count in counts.items()]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.pie(counts, labels=labels, autopct=\"\", startangle=90, wedgeprops=dict(width=0.5))\n",
    "    plt.title(f\"{trajectory}\\nTime Difference Distribution (n={n})\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ad88a-6530-4e87-8274-2fd030f36681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Second Classification!\n",
    "# Classification Strategy 2: Reclassify visit-based BP using matched ambulatory BP within 1 year (alternative to original classification)\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the merged file containing both visit (CSV2) and ambulatory (CSV1) BP records\n",
    "df = pd.read_csv(\"Desktop/CSV3_merged_all_patients_filled.csv\")\n",
    "\n",
    "# Ensure StartDate is datetime\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"], errors=\"coerce\")\n",
    "\n",
    "# Split into visit-based and ambulatory-based BP subsets\n",
    "visit_df = df[df[\"ItemOID_CSV2\"].notna()].copy()\n",
    "ambulatory_df = df[df[\"ItemOID_CSV1\"].notna()].copy()\n",
    "\n",
    "# Initialize result list\n",
    "result_rows = []\n",
    "\n",
    "# Loop through each visit record (CSV2-based)\n",
    "for idx, visit_row in visit_df.iterrows():\n",
    "    patient_id = visit_row[\"patient_ID\"]\n",
    "    subject_key = visit_row[\"SubjectKey\"]\n",
    "    visit_date = visit_row[\"StartDate\"]\n",
    "\n",
    "    # Find all ambulatory BP rows for the same patient\n",
    "    amb_subset = ambulatory_df[ambulatory_df[\"SubjectKey\"] == subject_key]\n",
    "\n",
    "    # Find ambulatory records within ±365 days of the visit\n",
    "    amb_within_year = amb_subset[\n",
    "        (amb_subset[\"StartDate\"] - visit_date).abs() <= timedelta(days=365)\n",
    "    ]\n",
    "\n",
    "    # Copy the visit row\n",
    "    combined_row = visit_row.copy()\n",
    "\n",
    "    # Add each qualifying ambulatory BP classification as a new column\n",
    "    for i, amb_row in amb_within_year.iterrows():\n",
    "        amb_date = amb_row[\"StartDate\"].strftime(\"%Y-%m-%d\")\n",
    "        new_col_name = f\"BP_Category_CSV1_{amb_date}\"\n",
    "        combined_row[new_col_name] = amb_row[\"BP_Category_CSV1\"]\n",
    "\n",
    "    # Append combined row to result\n",
    "    result_rows.append(combined_row)\n",
    "\n",
    "# Combine all rows into a final DataFrame\n",
    "final_result_df = pd.DataFrame(result_rows)\n",
    "\n",
    "# Save to CSV\n",
    "final_result_df.to_csv(\"Desktop/CSV3_BP_Matching_Result.csv\", index=False)\n",
    "\n",
    "print(\"Reclassification matching completed. Output saved to: Desktop/CSV3_BP_Matching_Result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993bb83-9508-4ba9-9c52-08d35442cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Strategy 2 (Nearest): For each visit BP record, match to the closest ambulatory BP record within 365 days\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the merged BP dataset\n",
    "df = pd.read_csv(\"Desktop/CSV3_merged_all_patients_filled.csv\")\n",
    "\n",
    "# Ensure StartDate is in datetime format\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"], errors=\"coerce\")\n",
    "\n",
    "# Split into visit-based and ambulatory-based subsets\n",
    "visit_df = df[df[\"ItemOID_CSV2\"].notna()].copy()\n",
    "ambulatory_df = df[df[\"ItemOID_CSV1\"].notna()].copy()\n",
    "\n",
    "# Initialize columns to store nearest match\n",
    "visit_df[\"BP_Category_CSV1_StartDate\"] = pd.NaT\n",
    "visit_df[\"BP_Category_CSV1_value\"] = None\n",
    "\n",
    "# Match each visit record to nearest ambulatory record within 1 year\n",
    "for idx, row in visit_df.iterrows():\n",
    "    subject = row[\"SubjectKey\"]\n",
    "    visit_date = row[\"StartDate\"]\n",
    "\n",
    "    # Filter ambulatory records for the same subject\n",
    "    candidates = ambulatory_df[ambulatory_df[\"SubjectKey\"] == subject].copy()\n",
    "    candidates[\"DateDiff\"] = (candidates[\"StartDate\"] - visit_date).abs()\n",
    "\n",
    "    # Keep only those within ±365 days\n",
    "    candidates = candidates[candidates[\"DateDiff\"] <= pd.Timedelta(days=365)]\n",
    "\n",
    "    if not candidates.empty:\n",
    "        # Select the record with the smallest time difference\n",
    "        nearest = candidates.sort_values(\"DateDiff\").iloc[0]\n",
    "        visit_df.at[idx, \"BP_Category_CSV1_StartDate\"] = nearest[\"StartDate\"]\n",
    "        visit_df.at[idx, \"BP_Category_CSV1_value\"] = nearest[\"BP_Category_CSV1\"]\n",
    "\n",
    "# Save result\n",
    "output_path = \"Desktop/CSV3_BP_Matching_Nearest.csv\"\n",
    "visit_df.to_csv(output_path, index=False)\n",
    "print(f\"Nearest match completed. File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf6cb9-522c-421d-b7c1-54ce77651493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Strategy 2 (Nearest): Match to the closest ambulatory BP within 365 days and record time difference\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the merged BP dataset\n",
    "df = pd.read_csv(\"Desktop/CSV3_merged_all_patients_filled.csv\")\n",
    "\n",
    "# Ensure StartDate is in datetime format\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"], errors=\"coerce\")\n",
    "\n",
    "# Split into visit-based and ambulatory-based subsets\n",
    "visit_df = df[df[\"ItemOID_CSV2\"].notna()].copy()\n",
    "ambulatory_df = df[df[\"ItemOID_CSV1\"].notna()].copy()\n",
    "\n",
    "# Initialize new columns\n",
    "visit_df[\"BP_Category_CSV1_StartDate\"] = pd.NaT\n",
    "visit_df[\"BP_Category_CSV1_value\"] = None\n",
    "visit_df[\"BP_Category_CSV1_time_difference_days\"] = None\n",
    "\n",
    "# Match each visit record to nearest ambulatory record and record time difference\n",
    "for idx, row in visit_df.iterrows():\n",
    "    subject = row[\"SubjectKey\"]\n",
    "    visit_date = row[\"StartDate\"]\n",
    "\n",
    "    # Filter ambulatory records for the same subject\n",
    "    candidates = ambulatory_df[ambulatory_df[\"SubjectKey\"] == subject].copy()\n",
    "    candidates[\"DateDiff\"] = (candidates[\"StartDate\"] - visit_date).abs()\n",
    "\n",
    "    # Keep only those within ±365 days\n",
    "    candidates = candidates[candidates[\"DateDiff\"] <= pd.Timedelta(days=365)]\n",
    "\n",
    "    if not candidates.empty:\n",
    "        nearest = candidates.sort_values(\"DateDiff\").iloc[0]\n",
    "        visit_df.at[idx, \"BP_Category_CSV1_StartDate\"] = nearest[\"StartDate\"]\n",
    "        visit_df.at[idx, \"BP_Category_CSV1_value\"] = nearest[\"BP_Category_CSV1\"]\n",
    "        visit_df.at[idx, \"BP_Category_CSV1_time_difference_days\"] = nearest[\"DateDiff\"].days\n",
    "\n",
    "# Save result\n",
    "output_path = \"Desktop/CSV3_BP_Matching_Nearest_WithTimeDiff.csv\"\n",
    "visit_df.to_csv(output_path, index=False)\n",
    "print(f\"Nearest match with time difference completed. File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f1c70-c2ed-4a6f-9d31-6726c368f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean matched dataset: remove unmatched rows and subjects with only one record\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load nearest matching result\n",
    "df = pd.read_csv(\"Desktop/CSV3_BP_Matching_Nearest_WithTimeDiff.csv\")\n",
    "\n",
    "# Step 1: Remove rows with no matched CSV1 values\n",
    "df = df[df[\"BP_Category_CSV1_value\"].notna()]\n",
    "\n",
    "# Step 2: Remove subjects with only one total record\n",
    "subject_counts = df[\"SubjectKey\"].value_counts()\n",
    "df = df[df[\"SubjectKey\"].isin(subject_counts[subject_counts > 1].index)]\n",
    "\n",
    "# Save cleaned file\n",
    "output_path = \"Desktop/CSV3_BP_Matching_Nearest_Cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcdd389-ce01-4f53-a641-6ef27a88c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign BP_Classification labels based on simplified CSV1 and CSV2 categories\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"Desktop/CSV3_BP_Matching_Nearest_Cleaned.csv\")\n",
    "\n",
    "# Convert full category names to short codes: H (High), N (Normal), L (Low)\n",
    "def simplify_category(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    val = val.strip().lower()\n",
    "    if \"high\" in val:\n",
    "        return \"H\"\n",
    "    elif \"normal\" in val:\n",
    "        return \"N\"\n",
    "    elif \"low\" in val:\n",
    "        return \"L\"\n",
    "    return None\n",
    "\n",
    "df[\"BP_CSV1_Short\"] = df[\"BP_Category_CSV1_value\"].apply(simplify_category)\n",
    "df[\"BP_CSV2_Short\"] = df[\"BP_Category_CSV2\"].apply(simplify_category)\n",
    "\n",
    "# Define classification mapping based on short codes\n",
    "classification_map = {\n",
    "    (\"H\", \"H\"): \"High BP\",\n",
    "    (\"H\", \"N\"): \"Masked Hypertension\",\n",
    "    (\"H\", \"L\"): \"Masked Hypertension\",\n",
    "    (\"N\", \"H\"): \"High BP White coat\",\n",
    "    (\"N\", \"N\"): \"Normal BP\",\n",
    "    (\"N\", \"L\"): \"Whitecoat Low BP\",\n",
    "    (\"L\", \"H\"): \"High BP White coat\",\n",
    "    (\"L\", \"N\"): \"Masked Low BP\",\n",
    "    (\"L\", \"L\"): \"Low BP\"\n",
    "}\n",
    "\n",
    "# Generate final classification column\n",
    "df[\"BP_Classification\"] = df.apply(\n",
    "    lambda row: classification_map.get((row[\"BP_CSV1_Short\"], row[\"BP_CSV2_Short\"]), \"Unclassified\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save final result\n",
    "output_path = \"Desktop/CSV3_BP_Matching_Final_With_Classification.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965c783-2f21-4ff4-805e-836f098ae38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart showing the proportion of patients with changed vs unchanged BP classification\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the final classification file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_Final_With_Classification.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure StartDate is in datetime format\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\n",
    "\n",
    "# Sort by SubjectKey and StartDate\n",
    "df = df.sort_values(by=[\"SubjectKey\", \"StartDate\"]).reset_index(drop=True)\n",
    "\n",
    "# Remove patients with only one record\n",
    "df = df[df.duplicated(subset=\"SubjectKey\", keep=False)].reset_index(drop=True)\n",
    "\n",
    "# Count unique BP_Classification types for each patient\n",
    "bp_change_status = df.groupby(\"SubjectKey\")[\"BP_Classification\"].nunique().reset_index()\n",
    "\n",
    "# Label as Changed if more than one type, else Unchanged\n",
    "bp_change_status[\"BP_Change_Status\"] = bp_change_status[\"BP_Classification\"].apply(\n",
    "    lambda x: \"Changed\" if x > 1 else \"Unchanged\"\n",
    ")\n",
    "\n",
    "# Plot pie chart\n",
    "status_counts = bp_change_status[\"BP_Change_Status\"].value_counts()\n",
    "labels = status_counts.index\n",
    "colors = [\"#66c2a5\", \"#fc8d62\"]\n",
    "explode = [0.05 if label == \"Changed\" else 0 for label in labels]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(status_counts, labels=labels, autopct=\"%1.1f%%\", colors=colors, explode=explode, startangle=90)\n",
    "plt.title(\"Proportion of BP Changed vs Unchanged Patients\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"Desktop/bp_change_pie_chart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb477032-e3d3-4014-bef3-4572594d9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trajectory labels based on BP_Classification_Final sorted by patient and visit time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the input file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Copy BP_Category_CSV1_value to a new column\n",
    "df[\"BP_Classification_Final\"] = df[\"BP_Category_CSV1_value\"]\n",
    "\n",
    "# Ensure StartDate is datetime\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\n",
    "\n",
    "# Sort by patient and date\n",
    "df_sorted = df.sort_values(by=[\"SubjectKey\", \"StartDate\"]).reset_index(drop=True)\n",
    "\n",
    "# Initialize new trajectory column\n",
    "df_sorted[\"Trajectory_Final\"] = \"\"\n",
    "\n",
    "# Assign trajectory between consecutive visits\n",
    "for subject, group in df_sorted.groupby(\"SubjectKey\"):\n",
    "    group = group.sort_values(by=\"StartDate\").reset_index()\n",
    "    for i in range(1, len(group)):\n",
    "        idx = group.loc[i, \"index\"]\n",
    "        current_bp = group.loc[i, \"BP_Classification_Final\"]\n",
    "        previous_bp = group.loc[i - 1, \"BP_Classification_Final\"]\n",
    "        trajectory = f\"{previous_bp} - {current_bp}\"\n",
    "        df_sorted.at[idx, \"Trajectory_Final\"] = trajectory\n",
    "\n",
    "# Save output\n",
    "output_path = \"Desktop/CSV3_BP_Matching_With_Trajectory_Final.csv\"\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e18097-a1bb-42ca-8091-dd85b44020b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transition age vs. time difference (days) for each of the 9 BP trajectory types using Final Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the final trajectory-labeled file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory_Final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure numeric format\n",
    "df[\"time_dif_visit_Days\"] = pd.to_numeric(df[\"time_dif_visit_Days\"], errors=\"coerce\")\n",
    "\n",
    "# Define 9 trajectory types and corresponding colors\n",
    "trajectory_list = [\n",
    "    (\"High BP - Normal BP\", \"blue\"),\n",
    "    (\"High BP - Low BP\", \"orange\"),\n",
    "    (\"High BP - High BP\", \"gold\"),\n",
    "    (\"Normal BP - High BP\", \"purple\"),\n",
    "    (\"Normal BP - Low BP\", \"green\"),\n",
    "    (\"Normal BP - Normal BP\", \"gray\"),\n",
    "    (\"Low BP - High BP\", \"brown\"),\n",
    "    (\"Low BP - Normal BP\", \"red\"),\n",
    "    (\"Low BP - Low BP\", \"teal\")\n",
    "]\n",
    "\n",
    "# Loop through each trajectory type and plot scatter plot\n",
    "for trajectory_type, color in trajectory_list:\n",
    "    filtered_df = df[df[\"Trajectory_Final\"] == trajectory_type].dropna(subset=[\"time_dif_visit_Days\", \"transition_age\"])\n",
    "    n = len(filtered_df)\n",
    "\n",
    "    if n == 0:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(filtered_df[\"transition_age\"], filtered_df[\"time_dif_visit_Days\"],\n",
    "                color=color, alpha=0.6)\n",
    "\n",
    "    max_y = filtered_df[\"time_dif_visit_Days\"].max()\n",
    "    yticks = np.arange(0, max_y + 500, 500)\n",
    "    plt.yticks(yticks)\n",
    "\n",
    "    plt.title(f\"{trajectory_type} (n = {n})\\nTransition Age vs. Time Difference Between Visits\")\n",
    "    plt.xlabel(\"Transition Age\")\n",
    "    plt.ylabel(\"Time Difference Between Visits (Days)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8bae6-300d-41e8-8050-f3c29fa84650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart based on BP_Classification proportions using final classification file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory_Final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Count BP_Classification types\n",
    "bp_counts = df[\"BP_Classification\"].value_counts()\n",
    "sizes = bp_counts.values\n",
    "labels_raw = bp_counts.index\n",
    "total = bp_counts.sum()\n",
    "\n",
    "# Labels with count and percentage\n",
    "labels_full = [f\"{label} (n={count}, {count/total*100:.1f}%)\" for label, count in zip(labels_raw, sizes)]\n",
    "legend_labels = [f\"{label} (n={count})\" for label, count in zip(labels_raw, sizes)]\n",
    "\n",
    "# Draw pie chart with annotations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "wedges, texts = ax.pie(\n",
    "    sizes,\n",
    "    startangle=140,\n",
    "    labeldistance=1.2,\n",
    "    wedgeprops=dict(width=0.5),\n",
    "    textprops=dict(size=12)\n",
    ")\n",
    "\n",
    "# Annotate each slice with arrow and text\n",
    "for i, wedge in enumerate(wedges):\n",
    "    angle = (wedge.theta2 + wedge.theta1) / 2\n",
    "    x = np.cos(np.deg2rad(angle))\n",
    "    y = np.sin(np.deg2rad(angle))\n",
    "    ha = 'left' if x >= 0 else 'right'\n",
    "    offset_y = (i % 2) * 0.05 if x < 0 else 0\n",
    "    \n",
    "    ax.annotate(\n",
    "        labels_full[i],\n",
    "        xy=(x * 0.8, y * 0.8),\n",
    "        xytext=(x * 1.3, y * 1.3 + offset_y),\n",
    "        arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "        ha=ha, va='center', fontsize=12\n",
    "    )\n",
    "\n",
    "# Add legend\n",
    "plt.legend(wedges, legend_labels, title=\"BP Classification\", loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=2, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9e6f7-4536-4063-85c3-00e2eb284733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the second classification method (Trajectory_Final), plot 9 scatterplots by BP trajectory type\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Load final trajectory classification file\n",
    "file_path = \"Desktop/CSV3_BP_Matching_With_Trajectory_Final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure correct numeric format\n",
    "df[\"time_dif_visit_Days\"] = pd.to_numeric(df[\"time_dif_visit_Days\"], errors=\"coerce\")\n",
    "\n",
    "# Define 9 trajectory types and colors\n",
    "trajectory_list = [\n",
    "    (\"High BP - Normal BP\", \"blue\"),\n",
    "    (\"High BP - Low BP\", \"orange\"),\n",
    "    (\"High BP - High BP\", \"gold\"),\n",
    "    (\"Normal BP - High BP\", \"purple\"),\n",
    "    (\"Normal BP - Low BP\", \"green\"),\n",
    "    (\"Normal BP - Normal BP\", \"gray\"),\n",
    "    (\"Low BP - High BP\", \"brown\"),\n",
    "    (\"Low BP - Normal BP\", \"red\"),\n",
    "    (\"Low BP - Low BP\", \"teal\")\n",
    "]\n",
    "\n",
    "# Determine axis ranges based on full data\n",
    "all_transition_age = df[\"transition_age\"].dropna()\n",
    "all_time_diff = df[\"time_dif_visit_Days\"].dropna()\n",
    "x_min = all_transition_age.min() - 2\n",
    "x_max = all_transition_age.max() + 2\n",
    "y_min = 0\n",
    "y_max = all_time_diff.max() + 500\n",
    "\n",
    "# Create 3x3 grid of subplots\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "gs = gridspec.GridSpec(3, 3, figure=fig)\n",
    "\n",
    "for i, (trajectory_type, color) in enumerate(trajectory_list):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    \n",
    "    filtered_df = df[df[\"Trajectory_Final\"] == trajectory_type].dropna(subset=[\"time_dif_visit_Days\", \"transition_age\"])\n",
    "    n = len(filtered_df)\n",
    "    \n",
    "    if n > 0:\n",
    "        ax.scatter(filtered_df[\"transition_age\"], filtered_df[\"time_dif_visit_Days\"], color=color, alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f\"{trajectory_type} (n={n})\", fontsize=12)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(\"Transition Age\")\n",
    "    ax.set_ylabel(\"Time Difference (Days)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle(\"Blood Pressure Trajectory Patterns (Trajectory_Final)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
